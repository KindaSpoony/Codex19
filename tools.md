# Tools

Investigative Tools & Modules Reference

OSINT Tools and Resources

We deploy a range of Open-Source Intelligence (OSINT) tools to gather information, with each tool used in a controlled, ethical manner. All data collection adheres to legal and ethical standards (no unauthorized access or hacking), and results are filtered for credibility per our source evaluation guidelines. Key tool categories include:
	•	Web Search Engines (e.g., Google, Bing): Purpose: Provide broad access to indexed web content for initial fact-finding or lead generation. Query Strategy: Use precise keywords and advanced operators (quoting exact phrases, using site: for domain-specific searches, - to exclude terms, etc.) to pinpoint relevant information. Credibility Integration: Search results are sifted through credibility filters — prioritizing reputable domains (official, academic, or well-established sources) and flagging low-quality sites. The LLM cross-verifies critical facts across multiple search hits, in line with source-evaluation rules, to ensure only reliable information is used.
	•	News & Media Archives: Tools: Online news aggregators and archives (e.g., Google News, Factiva, LexisNexis) that index newspapers, journals, and media reports. Purpose: Retrieve historical and current news articles or press releases relevant to the investigation. Query Strategy: Use date-range filters and specific media sources or keywords to find context, quotes, or background in credible news outlets. Credibility Integration: Results are filtered by source reputation (favoring mainstream, vetted media). The system applies credibility checks (as defined in source-evaluation.md) to identify potential bias or misinformation, ensuring cited media reports meet our reliability standards.
	•	Academic Databases: Tools: Scholarly search engines (e.g., Google Scholar, academic journal databases). Purpose: Find peer-reviewed papers, studies, or expert analyses that provide deep context or evidence on a subject. Query Strategy: Search by keywords, author names, or publication titles; use filters like filetype (PDF for papers) or site:.edu for university repositories. Credibility Integration: Academic sources are inherently high-credibility, but the LLM still evaluates the publication venue and author credentials. It cross-references findings with other literature, maintaining consistency and flagging any outlier claims. Any academic source is cited with proper context to preserve transparency for audit purposes.
	•	Public Records & Databases: Tools: Government databases, public record search portals, and official registries (e.g., court records, company registries, SEC EDGAR). Purpose: Obtain official data such as corporate filings, property records, court documents, or regulatory filings that can corroborate investigative leads. Query Strategy: Use targeted queries on government or institutional sites (often combining site:.gov or specific database search interfaces) and keywords like names, case numbers, or document types. Credibility Integration: Information from official records is treated as authoritative but is still checked for context and completeness. The LLM’s credibility filter gives high weight to these sources, while ensuring the data is up-to-date and matches other known records (to catch any forgeries or outdated entries).
	•	Social Media Search: Tools: Platform-specific search features and third-party OSINT tools for social content (e.g., Twitter Advanced Search, LinkedIn search, Reddit queries). Purpose: Collect posts, profiles, or social interactions that might provide leads, sentiments, or corroborating evidence in an investigation. Query Strategy: Use advanced search parameters (hashtags, mentions, date filters, or inurl: with profile/user IDs) to locate relevant social media posts or user data. Credibility Integration: Because social content can be volatile and unverified, the system employs strict credibility checks: verifying account authenticity (official vs. imposter), cross-checking claims made in posts against news or official sources, and noting the content’s emotional tone and potential bias (per heuristics.md guidance). Any social media-derived information is triangulated with other sources before being taken as fact.
	•	Web Caches & Archives: Tools: Cached page retrieval and web archives (e.g., Google Cache, Internet Archive’s Wayback Machine). Purpose: Access historical snapshots of web pages or content that might have been changed or taken down. This is crucial for auditing edits or retrieving evidence that has been deleted. Query Strategy: Use the cache: operator for recent Google-cached pages or search Archive.org by URL and date. Combine with keywords if looking for specific content within cached pages. Credibility Integration: Cached content inherits the credibility of the original site, but the module confirms the snapshot’s timestamp and checks if the content was altered since (to detect deletions or propaganda retrofits). The LLM will annotate findings with the retrieval date and use multiple archive sources if available, to ensure the page integrity is trustworthy and auditable.

LLM Operational Modules

To conduct investigations systematically, the AI (LLM) operates through specialized internal modules. Each module governs a facet of behavior or analysis, ensuring the AI’s workflow remains ethical, logical, and aligned with project standards (as detailed in our ethics.md, heuristics.md, and source-evaluation.md documents). Below are the key operational modules, with their function and purpose:
	•	Ethical/Policy Filter: Function: Monitors all queries, data retrievals, and outputs against the project’s ethical guidelines and policies (from ethics.md). It filters out or redacts any content that violates privacy, legal restrictions, or ethical standards. Operational Purpose: Ensures compliance and prevents the AI from engaging in disallowed activities (e.g. unauthorized surveillance, personal data leaks). This gate keeps the investigation audit-friendly and morally aligned, blocking any responses or actions that would breach our code of conduct.
	•	Logic Gates: Function: Implements conditional checkpoints in the AI’s reasoning process. At critical decision junctures, it uses predefined logic rules (from heuristics.md) to evaluate whether the next step is warranted. For example, before trusting a single source, a logic gate may require cross-validation by a second source. Operational Purpose: Maintains rigorous logical consistency and step-by-step validation. These gates prevent logical fallacies or premature conclusions by ensuring each inference passes sanity checks and follows from evidence. Essentially, they act as circuit breakers if the AI’s reasoning becomes unsound or jumps to unsupported conclusions.
	•	Source Evaluation Routines: Function: Automatically assesses each information source’s credibility and relevance using criteria set in source-evaluation.md. This module checks domain authority, source bias, date of publication, and corroboration status. Operational Purpose: Filters and ranks incoming data by trustworthiness. By scoring sources (e.g., preferring official documents and expert research over anonymous blogs), it ensures the AI’s analysis leans on high-quality evidence. It also logs source credibility metadata for each piece of information, supporting transparent reporting and enabling auditors to trace how conclusions were reached based on source quality.
	•	Triangulation Module: Function: Cross-verifies key facts and claims across multiple independent sources. Whenever the AI encounters a critical piece of information, this module prompts it to find at least two corroborating sources (e.g., a fact stated in a news article would be checked against official data or other news outlets). Operational Purpose: Increases the reliability of the investigation’s findings by avoiding single-source dependency. Triangulation uncovers discrepancies and confirms consistency, reducing the risk of being misled by false or biased information. It essentially provides a built-in fact-checking mechanism, where contradictory evidence triggers further analysis or flags uncertainties for review.
	•	Threat Pattern Detection: Function: Scans collected data for known patterns or indicators of threats relevant to the investigation (such as signs of fraud, cyber-attacks, violent intent, or other malicious activities depending on context). It utilizes pattern libraries and keywords (defined by prior cases or domain experts) to identify red flags in text, images, or metadata. Operational Purpose: Alerts the AI to potential risks or malicious actors early in the investigation. By detecting threat patterns (e.g., mentions of dangerous substances, extremist language, or attack planning terms), this module helps prioritize leads and lines of inquiry that might involve security issues. It ensures that the AI doesn’t overlook critical threat indicators and can escalate or apply caution as needed (consistent with safety protocols in ethics.md).
	•	Emotional Tone Gates: Function: Monitors the emotional and subjective tone of both source content and the AI’s own generated text. If a source document exhibits strong bias or emotive language, the module flags it for careful handling. Similarly, it checks the AI’s draft outputs to ensure a neutral, professional tone. Operational Purpose: Maintains objectivity and analytical clarity. In practice, this gate prevents overly emotional or biased content from unduly influencing the investigation’s conclusions or the AI’s responses. It prompts rephrasing or additional context if the AI’s tone becomes too influenced by charged language, thus upholding a consistent, impartial voice in line with our reporting standards.
	•	Confidence Intervaling: Function: Evaluates the certainty level of the AI’s findings and assertions. For each answer or conclusion, the module requires an internal “confidence score” or interval, based on factors like evidence strength, source agreement, and data completeness. It may use techniques like self-consistency checks or reference class forecasting to quantify uncertainty. Operational Purpose: Provides transparency about how much trust to place in a given result. By explicitly labeling conclusions with confidence levels (e.g., high, medium, low confidence) or probability estimates, the AI helps investigators gauge where further verification is needed. This fosters an audit trail where every key point is not only supported by sources but also tagged with the AI’s confidence, aligning with an evidence-driven approach.
	•	Historical Consistency Check: Function: Verifies that information and assertions remain consistent over time and with known historical facts. The module cross-references timeline data — ensuring that events are reported in correct chronological order and that no new finding contradicts previously established facts unless justified. It also checks the AI’s output against known historical records to catch anachronisms or factual errors. Operational Purpose: Prevents temporal errors and maintains narrative coherence throughout an investigation. If the AI uncovers new data that conflicts with earlier verified info or established history, this module forces a reconciliation step (either explaining the discrepancy, updating the narrative with context, or discarding the unreliable piece). This yields a final analysis that is chronologically sound and logically coherent, crucial for credibility and accuracy in reports.

Google Dorking Toolkit

When standard search techniques aren’t enough, we employ advanced Google search operators (often called “Google Dorking”) to uncover hard-to-find information. These operators allow targeted, powerful queries that can retrieve specific file types, find information within URLs or titles, access cached pages, and more. The following table outlines key advanced operators, their syntax, investigative purpose, and example use cases:

Operator & Category	Syntax Usage	Investigative Purpose	Example Query
File Type Search	filetype:<ext> <keywords>	Find specific document types (PDFs, DOCX, XLS, etc.) containing relevant keywords. Useful for locating reports, presentations, or data dumps not easily found via normal web browsing.	filetype:pdf site:example.gov "annual budget 2023" // searches for PDF documents on a government site containing “annual budget 2023”
In-URL Keyword	inurl:<term>	Find pages with certain terms in the URL. Often used to discover portals, login pages, or pages by content type (e.g., inurl:login, inurl:profile for user profiles). Can uncover hidden site sections or relevant query results on specific platforms.	inurl:report site:example.com "Q4 2024" // finds pages on example.com with “report” in the URL, likely targeting Q4 2024 reports
In-Title Keyword	intitle:<term>	Find pages with specific words in the HTML title. Helps locate pages focused on a topic (since titles summarize content) — for example, finding a page titled “Investigation Report” or a profile name.	intitle:"Employee Directory" company name // searches for pages likely to be an employee directory for the specified company
Cache Retrieval	cache:<URL>	Retrieve Google’s cached snapshot of a page. Allows viewing content that might have changed or been removed on the live site. This is useful for accessing pages no longer available or to see earlier versions of content.	cache:example.com "policy update" // views the cached version of example.com and highlights the phrase “policy update” if present, revealing content before recent changes
Site + Filetype Combo	site:<domain> filetype:<ext> <keywords>	Combine site restriction with file type to target specific repositories of information. Great for digging into a particular website (or domain category) for relevant documents. This finds files of interest within a known trusted domain (or across all sites with a certain TLD).	site:.gov filetype:xls "cybersecurity budget" // searches all .gov websites for Excel spreadsheets related to cybersecurity budgets

Each of these Google dorking techniques must be used judiciously. They can reveal valuable data, but we ensure the queries remain within ethical boundaries – focusing only on publicly available information. The LLM uses these operators in tandem with the above credibility filters and modules (e.g., checking that any discovered file is from a legitimate public source and not private data). All findings via dorking are cross-checked and documented to maintain an audit trail consistent with our investigative standards.